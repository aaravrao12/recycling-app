# Import necessary libraries
import os  # For file and directory operations
import numpy as np  # For numerical computations
import tensorflow as tf
import matplotlib.pyplot as plt  # For plotting (used in visualizing the confusion matrix)
from sklearn.model_selection import train_test_split  # For splitting the dataset into train, validation, and test sets
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # For creating and displaying the confusion matrix
from tensorflow.keras.preprocessing import image  # For image preprocessing
from tensorflow.keras.applications import EfficientNetB0  # Pre-trained EfficientNetB0 CNN model
from tensorflow.keras.applications.efficientnet import preprocess_input  # For pre-processing inputs as required by EfficientNetB0
from tensorflow.keras.models import Sequential, load_model  # To define a Sequential model and load existing models
from tensorflow.keras.layers import Dense, Flatten, Dropout, LayerNormalization  # Different layers for the neural network
from tensorflow.keras.utils import to_categorical  # To one-hot encode the labels (although not used here)
from sklearn.preprocessing import LabelEncoder  # To encode string labels to integer format
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For augmenting image data
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback  # For adding callbacks (stopping early, saving best model, etc.)
from tensorflow.keras.regularizers import l2  # For L2 regularization (to prevent overfitting)
from tensorflow.keras.optimizers import Adam  # Optimizer for model training
from google.colab import drive  # To mount Google Drive (used to access dataset in Google Colab)
import time  # For timing epochs in the training process

# Mount Google Drive to access the dataset
drive.mount('/content/drive')

# Define the paths to the folders where the images are stored (Recyclable, Non-Recyclable, Contaminated)
folder_paths = {
    'Recyclable': r"/content/drive/MyDrive/Recyclable/Recyclables",
    'Non-Recyclable': r"/content/drive/MyDrive/Non_Recyclable/NonRecyclable",
    'Contaminated': r"/content/drive/MyDrive/Contaminated_Recyclables/Contaminated"
}

# Create empty lists to store images and their corresponding labels
images, labels = [], []

# Load images and their labels from the defined folder paths
for label, folder_path in folder_paths.items():
    if os.path.exists(folder_path):  # Check if the folder path exists
        image_files = os.listdir(folder_path)  # List all files in the folder
        for image_file in image_files:
            img_path = os.path.join(folder_path, image_file)  # Get the full path of each image
            if os.path.isfile(img_path):  # Ensure it's a file and not a directory
                img = image.load_img(img_path, target_size=(512, 512))  # Load the image with the target size (512x512)
                img_array = image.img_to_array(img)  # Convert the image to a NumPy array
                images.append(img_array)  # Append the image data to the list
                labels.append(label)  # Append the corresponding label

# Convert the lists of images and labels into NumPy arrays
X = np.array(images)
y = np.array(labels)

# Preprocess the input data for EfficientNetB0
X = preprocess_input(X)  # Standardize the pixel values as required by the EfficientNetB0 model

# Encode the string labels (Recyclable, Non-Recyclable, Contaminated) into integers
le = LabelEncoder()
y_encoded = le.fit_transform(y)  # Converts the labels into integer-encoded values

# Split the data into training (70%), validation (15%), and testing (15%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.3, random_state=42)  # First split (train and temp)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp into validation and test

# Data augmentation to artificially increase the diversity of the training data
datagen = ImageDataGenerator(
    rotation_range=30,  # Random rotation of images up to 30 degrees
    width_shift_range=0.2,  # Horizontal shift
    height_shift_range=0.2,  # Vertical shift
    shear_range=0.2,  # Shearing transformation
    zoom_range=0.2,  # Random zoom
    horizontal_flip=True,  # Randomly flip images horizontally
    vertical_flip=True,  # Randomly flip images vertically
    brightness_range=[0.8, 1.2],  # Adjust brightness randomly
    fill_mode='nearest'  # Fill in missing pixels with the nearest value
)

# Define a custom callback to log time and progress for each epoch
class CustomLoggingCallback(Callback):
    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_start_time = time.time()  # Record the start time of the epoch

    def on_epoch_end(self, epoch, logs=None):
        duration = time.time() - self.epoch_start_time  # Calculate the duration of the epoch
        logs = logs or {}
        # Print a summary of each epoch's performance (steps, duration, accuracy, loss, validation accuracy, etc.)
        output = (
            f"{self.params['steps']} - "
            f"{duration:.0f}s - "
            f"accuracy: {logs.get('accuracy'):.4f} - "
            f"loss: {logs.get('loss'):.4f} - "
            f"val_accuracy: {logs.get('val_accuracy'):.4f} - "
            f"val_loss: {logs.get('val_loss'):.4f}"
        )
        print(f"Epoch {epoch + 1}/{self.params['epochs']} - {output}")

# Define the CNN model architecture
model = Sequential()
model.add(EfficientNetB0(weights='imagenet', include_top=False, input_shape=(512, 512, 3)))  # Add pre-trained EfficientNetB0 without the top layers
model.add(Flatten())  # Flatten the output of EfficientNetB0 into a 1D vector
model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))  # Fully connected layer with L2 regularization to avoid overfitting
model.add(LayerNormalization())  # Use LayerNormalization instead of BatchNormalization
model.add(Dropout(0.4))  # Dropout layer to reduce overfitting by randomly setting 40% of input units to 0
model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))  # Output layer with softmax activation for multi-class classification

# Compile the model using Adam optimizer with a low learning rate, sparse categorical crossentropy for multi-class classification
model.compile(optimizer=Adam(learning_rate=0.0001),  # Use Adam optimizer with a reduced learning rate
              loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification with integer labels
              metrics=['accuracy'])  # Track accuracy as the performance metric

# Define early stopping and model checkpoint callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)  # Stop if validation loss doesn't improve for 5 epochs
model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')  # Save the model with the lowest validation loss

# Train the model using the augmented data, with early stopping and checkpointing
model.fit(datagen.flow(X_train, y_train, batch_size=64),  # Train the model using data augmentation
          epochs=10,  # Train for up to 10 epochs
          validation_data=(X_val, y_val),  # Use validation data to monitor performance
          callbacks=[early_stopping, model_checkpoint, CustomLoggingCallback()],  # Use callbacks for logging, early stopping, and saving the best model
          verbose=0)  # Suppress verbose output (only show custom logs)

# Save your model here
model.save('/content/drive/MyDrive/my_trained_model(1).h5', save_format='h5')  # Save the trained model to a file

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test, y_test)  # Get the loss and accuracy on the test set
print(f'Accuracy: {accuracy:.4f}')  # Print the accuracy

# Make predictions on the test set
y_pred = np.argmax(model.predict(X_test), axis=1)  # Predict classes and convert predictions to label indices

# Create a confusion matrix to evaluate classification performance
cm = confusion_matrix(y_test, y_pred)  # Generate confusion matrix from true and predicted labels

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)  # Display labels as the original class names
disp.plot(cmap=plt.cm.Blues)  # Plot confusion matrix using a blue color map
plt.title("Confusion Matrix")  # Add a title to the plot
plt.show()  # Show the confusion matrix plot

# Load the original model for re-saving
original_model_path = '/content/drive/MyDrive/my_trained_model(1).h5'  # Update this path if needed
original_model = load_model(original_model_path)  # Load the original model

# Create a new model architecture without BatchNormalization
new_model = Sequential()
new_model.add(EfficientNetB0(weights='imagenet', include_top=False, input_shape=(512, 512, 3)))  # Use the same base model
new_model.add(Flatten())  # Flatten the output from the base model
new_model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))  # Add a dense layer
new_model.add(LayerNormalization())  # Use LayerNormalization instead of BatchNormalization
new_model.add(Dropout(0.4))  # Add a dropout layer
new_model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))  # Output layer for classification

# Transfer weights from the original model to the new model (excluding BatchNormalization layers)
for original_layer, new_layer in zip(original_model.layers, new_model.layers):
    if not isinstance(original_layer, tf.keras.layers.BatchNormalization):  # Check if the layer is not BatchNormalization
        new_layer.set_weights(original_layer.get_weights())  # Transfer weights

# Save the new model in .keras format
new_model.save('/content/drive/MyDrive/my_simplified_model.keras', save_format='keras')
print("New model saved without BatchNormalization in .keras format.")
